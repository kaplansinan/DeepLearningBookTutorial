{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## RNN and LSTM\n",
    "This notebook covers the details of RCNN and LSTM alongside with their application area.\n",
    "\n",
    "Sequential models are used to infer sequential data in which a sequence of information follows another one. To understand the intuition behind this topic, let us take an example of a bouncing ball. To estimate the next location of the ball, we need to remember where it was before. SO sequential data is like that, just one follows another in order. For instance, audio and text data are sequential data samples.To be able to model that type of data and apply deep learning model, one should take into account a memory module to keep past info. \n",
    "\n",
    "[TODO:add bouncing ball figure].\n",
    "\n",
    "To uncover the usage of deep learning in sequential data modeling, this notebook dives in the details of widely used two model, Recurrent Neural Networks and Long-Short Term Memories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recurrent Neural Networks (RNN)\n",
    "Recurrent Neural Nets introduce sequential memory to keep past instance while inferring the current data. The question to answer is here that how RNN adds past information whilst processing the current instance. This is solved by a loop mechanism that use both previous inference and current instance. The below figure illustrates the working mechanism of RNNs. The hidden state keeps the information from previous iteration.\n",
    "\n",
    "<img src=\"fig/rnn.gif\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although RNN becomes quite handy to work sequential data. There are major issues with RNNs, which are:\n",
    "* short-term memory\n",
    "* vanishing gradients.\n",
    "\n",
    "Short-term memory problem occurs due to the vanishing gradients. That troubles RNN with retaining information from previous steps.\n",
    "\n",
    "To have a clear picture of the vanishing gradient problem, let us first examine how the backpropagation works in neural networks. \n",
    "\n",
    "As a rule of thumb, training of neural nets has three major steps stated below:\n",
    "1. performs forward pass based on given input and makes a prediction\n",
    "2. compare prediction to ground truth using a loss function, which outputs an error value\n",
    "3. It uses error value to perform backpropagation that computes gradients for each node in network\n",
    "\n",
    "Hence, gradient is used to adjust weights across a neural network. The bigger the gradient the bigger the adjustments become or vice versa. Very at this moment, a problem appears. When applying backpropagation, each node in a layer of the network computes it's gradients w.r.t. the gradients in the previous layer. Thus, if the adjustments to the layers before it is small the adjustment to the current layer will be even smaller. That causes gradient to exponentially shrink. Basically, the layer fails to learn anything.Due to this issue, RNN is not able to learn from an earlier steps.\n",
    "\n",
    "Luckily, there are methods to overcome the issues stated above. In this notebook, we cover two of these methods, Long-Short Term Memories (LSTM) and Gated Recurrent Units (GRU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and GRU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN suffers from short-term memory. For instance, if we have a long text to process, RNN may drop important information on the way. LSTM and GRU are introduced as a solution for this problem. They are capable of learning long-term dependencies. \n",
    "\n",
    "To have a clear picture of LSTM and GRU, imagine a situation in which you decided to bu a Lord of the Rings board game from amazon or any other online shops. The first thing you do while searching for the game is to read reviews from others who bought the same game. When reading reviews, your brain subconsciously only remembers key words like 'bad','good', 'fun'. You basically don't pay attention to whole sentences or verbs. If a friend asks you another day about what you read on the reviews, you most probably won't remember them sentence by sentence rather only key words. So some words will fade away. This is essentially what LSTM and GRU do. They only keep the relevant information.\n",
    "\n",
    "<img src=\"fig/LSTM.png\" width=500 height=500 />\n",
    "\n",
    "The key idea used in LSTM and GRU is ***cell state*** which keeps important information and drops unnecessary information alongside the way. One can imagine ***cell state*** like a conveyor belt. In addition to cell state, there are three gates available in LSTM, which are:\n",
    "* Forget gate \n",
    "* Input gate\n",
    "* Output gate. \n",
    "\n",
    "The gates plays and important role in keeping which data in sequence is important to keep or throw away unnecessary information. The gates are different neural networks that decide what information is allowed to on the cell state. To do so, gates contain **sigmoid function**. \n",
    "\n",
    "Sigmoid squashes values between 0 an 1. Thus, this is helpful to update or forget the data. Any data multiplied by zero is disappeared alongside the way. As a result, sigmoid makes the model learn which data to keep or forget. \n",
    "\n",
    "\n",
    "1. Forget Gate\n",
    "\n",
    "This gate basically decides what info thrown away or kept. Information from previous hidden state and information from the current input are passed through the sigmoid function. \n",
    "\n",
    "<img src=\"fig/forget_gate_lstm.gif\" width=500 height=500 />\n",
    "\n",
    "2. Input Gate\n",
    "\n",
    "This gate updates the cell state. First we pass previous hidden state and current input into a sigmoid function to decide what values will be updated. Afterwards, we also pass hidden state and current input into the tanh function to squish values b/w -1 and 1 to regulate the network. Then we multiply the output of the tanh function with the sigmoid output to decide which information shoul be kept and thrown away. \n",
    "\n",
    "<img src=\"fig/input_gate_lstm.gif\" width=500 height=500 />\n",
    "\n",
    "3. Cell State\n",
    "\n",
    "Now we have enough information from forget gate and input gate to calculate the cell state. First, the cell state is multiplied pointwise by the forget vector. This drops unnecessary information on the cell state. Then we take the output from input gate and do a pointwise addition to update the cell state with new information. \n",
    "\n",
    "<img src=\"fig/cellstate_lstm.gif\" width=500 height=500 />\n",
    "\n",
    "\n",
    "4. Output Gate\n",
    "\n",
    "At last we have output gate to transfer information from current step to next step. This gate determines what next hidden  state would be. For the sake of clarity, hidden state contains information from previous state. It is also used for predictions. The output gate is illustrated below and it basically filters and regulates the data with sigmoid and tanh function. \n",
    "\n",
    "<img src=\"fig/forget_gate_lstm.gif\" width=500 height=500 />\n",
    "\n",
    "Once the output gate transforms the information from previous gates, the new cell state and new hidden state are carried over the next time steps.\n",
    "\n",
    "To review, the gates have following functionalities:\n",
    "* Forget gate: to keep relevant information from prior states.\n",
    "* Input gate: to decide what information is relevant from the current step.\n",
    "* Output gate: to determine what next hidden state would be. \n",
    "\n",
    "#### GRU\n",
    "\n",
    "It is similar to LSTM but it gets rid of the cell state and use the hidden state to transfer information. It has two gates, reset and update. It is faster than LSTM but accuracy can vary based on an application domain. \n",
    "\n",
    "<img src=\"fig/GRU_gates.png\" width=500 height=500 />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit",
   "language": "python",
   "name": "python361064bit44046df9ee9f4b6794be314555d2632c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
