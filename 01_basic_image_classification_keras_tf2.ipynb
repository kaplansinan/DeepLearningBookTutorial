{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sequential Keras Model \n",
    " <hr>\n",
    "<code>\n",
    "from tf.keras.models import Sequential\n",
    "from tf.keras.layers import Conv2, MaxPooling2D, Flatten, Dense\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional API\n",
    "<hr>\n",
    "<code>  \n",
    "from tf.keras.models import Model\n",
    "from tf.keras.layers import Input, Conv2, MaxPooling2D, Flatten, Dense\n",
    "inputs = Input(shape=(32, 32, 3))\n",
    "x = Conv2D(32, (3, 3), activation='relu')(inputs)\n",
    "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "horizontal line\n",
    "<hr> \n",
    "<ol><li>1121232</li></ol>\n",
    "\n",
    "<code>\n",
    "    model.fit(train_dataset,\n",
    "          epochs=60,\n",
    "          validation_data=test_dataset,\n",
    "          validation_freq=1)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a data pipeline\n",
    "For each model we define in Keras, we need a data pipeline to feed it labeled training data. A data pipeline performs the following tasks:\n",
    "\n",
    "* **Loading:** Copying the dataset (e.g. images and labels) from storage into the program's memory.\n",
    "* __Preprocessing:__ transforming the dataset. For example, in image classification, we might resize, whiten, shuffle, or batch images.\n",
    "* <b>Feeding:</b> shoveling examples from a dataset into a training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data from storage\n",
    "First, we load CIFAR-10 from storage into numpy ndarrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple configs for data pipeline\n",
    "\n",
    "NUM_GPUS = 2\n",
    "BS_PER_GPU = 128\n",
    "NUM_EPOCHS = 60\n",
    "\n",
    "HEIGHT = 32\n",
    "WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 10\n",
    "NUM_TRAIN_SAMPLES = 50000\n",
    "\n",
    "BASE_LEARNING_RATE = 0.1\n",
    "LR_SCHEDULE = [(0.1, 30), (0.01, 45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, y), (x_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "(50000, 32, 32, 3) (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#debug dataset\n",
    "print(type(x), type(y))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this point we can directly feed this data into training. However, to get more accurate model we need to preprocess the data before feeding into the model. To do so, we will build data preprocessing pipeline to achieve this. We will be applying **tf.Data** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tf.data.Dataset class\n",
    "The TensorFlow Dataset class serves two main purposes:\n",
    "\n",
    "* It acts as a container that holds training data.\n",
    "* It can be used to perform alterations on elements of the training data.\n",
    "\n",
    "We instantiate a tensorflow.data.Dataset object for the CIFAR-10 dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9153819625526386467\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Dataset class.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the CIFAR-10 training examples stored in **train_dataset** will be accessed via the **take()** iterator:\n",
    "\n",
    "```\n",
    "for image, label in train_dataset.take(1):\n",
    "    (image.shape, label.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3) (1,)\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_dataset.take(1):\n",
    "    print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing steps to apply\n",
    "\n",
    "1. Data augmentation\n",
    "Augmentation is often used to \"inflate\" training datasets, which can improve generalization performance.\n",
    "\n",
    "Let's augment the CIFAR-10 dataset by performing the following steps on every image:\n",
    "\n",
    "1. Pad the image with a black, four-pixel border.\n",
    "2. Randomly crop a 32 x 32 region from the padded image.\n",
    "3. Flip a coin to determine if the image should be horizontally flipped.\n",
    "\n",
    "We achieve this by first defining a function that, given an image, performs the Steps 1-3 above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, HEIGHT + 8, WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we call the method **map**.\n",
    "\n",
    "This call returns a new Dataset object that contains the result of passing each image in CIFAR-10 into augmentation. This new object will emit transformed images in the original order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Shuffling\n",
    "\n",
    "We randomly shuffle the dataset. ***TensorFlow Dataset has a shuffle method***, which can be chained to our augmentation as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_dataset\n",
    "                 .map(augmentation)\n",
    "                 .shuffle(buffer_size=50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalization\n",
    "\n",
    "It's common practice to normalize data. Here, define a function that linearly scales each image to have zero mean and unit variance:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x, y):\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we chain it with our augmentation and shuffling operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\dev\\condapyp\\anaconda3\\envs\\tf2-cpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\image_ops_impl.py:1518: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = (train_dataset\n",
    "                 .map(augmentation)\n",
    "                 .shuffle(buffer_size=50000)\n",
    "                 .map(normalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Batching\n",
    "\n",
    "Finally, we batch the dataset. We set drop_remainder to True to remove enough training examples so that the training set's size is divisible by batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FULL PIPELINE\n",
    "train_dataset = (train_dataset.map(augmentation)\n",
    "                 .map(normalize)\n",
    "                 .shuffle(50000)\n",
    "                 .batch(128, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "A Keras model needs to be compiled before training. Compilation essentially defines three things: the loss function, the optimizer and the metrics for evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "model.compile(\n",
    "          loss='sparse_categorical_crossentropy',\n",
    "          optimizer=keras.optimizers.SGD(learning_rate=0.1, momentum=0.9),\n",
    "          metrics=['accuracy'])\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras uses the fit API to train a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.fit(train_dataset,\n",
    "          epochs=60,\n",
    "          validation_data=test_dataset,\n",
    "          validation_freq=1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, call the evaluate method with the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "model.evaluate(test_loader)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding callbacks\n",
    "Often we need to perform custom operations during training. For example, you might want to log statistics during the training for debugging or optimization purposes; implement a learning rate schedule to improve the efficiency of training; or save visual snapshots of filter banks as they converge. In TensorFlow 2, you can use the callback feature to implement customized events during training.\n",
    "\n",
    "Tensorboard\n",
    "\n",
    "TensorBoard is mainly used to log and visualize information during training. It is handy for examining the performance of the model. Tensorboard support is provided via the ***tensorflow.keras.callbacks.TensorBoard*** callback function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tensorboard_callback = TensorBoard(\n",
    "    log_dir=log_dir, update_freq='batch', histogram_freq=1)\n",
    "  \n",
    "model.fit(...,\n",
    "          callbacks=[tensorboard_callback])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedule\n",
    "\n",
    "Often, we would like to have fine control of learning rate as the training progresses. A custom learning rate schedule can be implemented as callback functions. Here, we create a customized schedule function that decreases the learning rate using a step function (at 30th epoch and 45th epoch). This schedule is converted to a ***keras.callbacks.LearningRateScheduler*** and attached to the **fit** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_LEARNING_RATE = 0.1\n",
    "LR_SCHEDULE = [(0.1, 30), (0.01, 45)]\n",
    "BS_PER_GPU = 128\n",
    "def schedule(epoch):\n",
    "  initial_learning_rate = BASE_LEARNING_RATE * BS_PER_GPU / 128\n",
    "  learning_rate = initial_learning_rate\n",
    "  for mult, start_epoch in LR_SCHEDULE:\n",
    "    if epoch >= start_epoch:\n",
    "      learning_rate = initial_learning_rate * mult\n",
    "    else:\n",
    "      break\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "lr_schedule_callback = LearningRateScheduler(schedule)\n",
    "\n",
    "model.fit(...,\n",
    "          callbacks=[..., lr_schedule_callback])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "This tutorial explains the basic of TensorFlow 2.0 with image classification as an example. The important things to remember are:\n",
    "\n",
    "* Data pipeline with TensorFlow 2's dataset API\n",
    "* Train, evaluation, save and restore models with Keras (TensorFlow 2's official high-level API)\n",
    "* Customized training with callbacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
