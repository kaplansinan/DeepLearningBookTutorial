{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex symbol\n",
    "\n",
    "http://www.rpi.edu/dept/arc/training/latex/LaTeX_symbols.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTIMIZATION FOR TRAINING DEEP LEARNING\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Optimization in deep learning involves finding set of parameters $\\theta$ for a model to reduce a cost function $J(\\theta)$. Before diving into the details of different optimization methods used in deep learning, it would be better to understand why the optimization is needed at the first hand, what kind of challenges exist there. \n",
    "\n",
    "To begin with, we define a general cost function to be optimized as follows:\n",
    "\n",
    "$$\\color{orange}{\\mathbb{E}_{(x,y) \\sim\\hat{p}_{data}} L(f(x;\\theta),y)\\tag{1}}$$\n",
    "\n",
    "where $L$ is the loss function, $f(x;\\theta)$ is the predicted output given input $x$, and $\\hat{p}_{data}$ is the empirical distribution. \n",
    "\n",
    "As the goal of deep learning models is to reduce the expected generalization error given by above equation, we can minimize the expected loss on the training set. This has to be done on training set since the true distribution of the data is unknown. \n",
    "\n",
    "$$\\color{orange}{\\mathbb{E}_{(x,y) \\sim\\hat{p}_{data}} L(f(x;\\theta),y) = \\frac{1}{m}\\sum^m_{i=1}L(f(x^{(i)};\\theta),y^{(i)})\\label{eq:error_minimize} \\tag{2}}$$\n",
    "\n",
    "where $m$ is the number of training examples. \n",
    "\n",
    "The nature of the cost/objective function given in  equation $\\eqref{eq:error_minimize}$ is important for optimization task. In some optimizations problems, the objective-function is just too complicated to evaluate directly at every iteration, and in this case, we use surrogate functions, functions that mimic most of the properties of the true objective-function, but that is much simpler analytically and/or computationally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Surrogate Loss Functions\n",
    "\n",
    "The main tool used by researchers to produce the result in machine learning is the optimization of the loss function (or sometimes quality function). However, the nature or the experiment being modeled doesn't always allows us to construct the loss function conveniently, for example as  {0,1}  for fail/success respectively. Instead if this, the surrogate models are used. The underlying principle can be formulated as follows: make the result computationally convenient without losing the sense.\n",
    "\n",
    "Ideally, your loss function should be the true loss, in real life. In a business, your true loss function is: -profit over the lifetime of the business. A lot of time it's difficult to measure the true loss, so your loss function should be a \"surrogate\" (approximation) of the true loss.\n",
    "\n",
    "For instance, below figure illustrates some loss function including $0-1$ loss. We may take an example of the Hinge loss plotted in the figure for a positive example. One useful property\n",
    "of hinge loss is that it is an upper bound on 0–1 loss; this is a useful property for a surrogate loss function, since it means that if you make the hinge loss small, you have  also made 0–1 loss small. \n",
    "\n",
    "<a href=\"https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/readings/L04%20Training%20a%20Classifier.pdf\"><img src=\"fig/loss_functions.png\" width=300 height=300 /></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that  a machine learning algorithm usually minimizes\n",
    "a surrogate loss function but halts when a convergence criterion based on **early\n",
    "stopping** is satisfied. Typically the early stopping criterion is based\n",
    "on the true underlying loss function, such as 0-1 loss measured on a validation set,\n",
    "and is designed to cause the algorithm to halt whenever overfitting begins to occur.\n",
    "Training often halts while the surrogate loss function still has large derivatives,\n",
    "which is very different from the pure optimization setting, where an optimization\n",
    "algorithm is considered to have converged when the gradient becomes very small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Batch and Minibatch Algorithms\n",
    "\n",
    "The optimization algorithms compute gradient each update using only subset of training set. The gradient and the optimization algorithms are explained in details at later sections of this notebook.\n",
    "\n",
    "The optimization algorithms are sorted into three categories given below:\n",
    "\n",
    "* Batch Algorithms: These are the algorithms that use entire training set for gradient update. They are also called deterministic gradient methods. \n",
    "\n",
    "* Stochastic Algorithms: These are the methods that use a single example at a time to compute gradient. \n",
    "\n",
    "* Minibatch Stochastic Methods: These are the methods that use more than one sample but fewer than entire training set to compute gradient. \n",
    "\n",
    "The most commonly used methods in deep learning belong to minibatch stochoastic family. These type of methods require an optimally selected minibatch size to compute gradient accordingly. There are different factors to consider while choosing an optimal minibatch size which are: \n",
    "* Larger batches provide a more accurate estimate of gradient.\n",
    "* Multicore hardware architectures usually underutilized with small batches. \n",
    "* When using GPU, it is common to select power of 2 batch sizes for better runtime.\n",
    "* Small batches offer regularization effect. However, in this case it is crucial to choose small learning rate to maintain the stability which is affected by the high variance in the estimate of the gradient. Also, once should except the training time \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Challenges in Neural Network Optimization\n",
    "\n",
    "It is a fact that each optimization task brings challenges on the table in machine learning. The problems encountered mostly during optimization of any ML task can be seen in optimization of deep learning models too. These challenges are:\n",
    "\n",
    "* Ill conditioning\n",
    "* Local Minima\n",
    "* Plateues, Saddle Points and Other Flat Regions\n",
    "* Cliffs and Exploding Gradients\n",
    "* Inexact Gradients\n",
    "* Poor Correspondence between local and global structure\n",
    "[TODO] Expain them in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Optimization Algorithms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Intuitive Explanation  of Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is a significant term that plays an important role in how the deep learning models learn over time. All the optimization algorithms used in deep learning are based on **gradient descent**. As the name suggests, Gradient means “slope” & Descent means “to go down”. Gradient Descent follows the slope of the curve and descends(“goes down”) to those combination of parameters in the function which gives the minimum value.\n",
    "\n",
    "Intuitively speaking, let's consider a path of a river originating from top of a mountain as shown below picture. \n",
    "\n",
    "<img src=\"fig/river.JPG\" width=500 height=500 />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of gradient descent is exactly what the river strives to achieve - namely, reach the bottom most point (at the foothill) climbing down from the mountain.\n",
    "\n",
    "Now, if the terrain of the mountain is shaped in such a way that the river doesn't have to stop anywhere completely before arriving at its final destination (which is the lowest point at the foothill, then this is the ideal case we desire. In deep learning, this is equivalent to saying that the global minimum (or optimum) of the solution starting from the initial point (top of the hill) is reached.\n",
    "\n",
    "However, it could be that the nature of terrain forces several pits in the path of the river, which could force the river to get trapped and stagnate. In deep learning terms, such pits are termed as local minima solutions, which is not desirable. Luckily, we can get rid of this with different ways that are discussed in details further. \n",
    "\n",
    "Gradient Descent therefore is prone to be stuck in local minimum, depending on the nature of the terrain. But, when you have a special kind of mountain terrain (which is shaped like a bowl, in math terms this is called a Convex Function), the algorithm is always guaranteed to find the optimum.\n",
    "\n",
    "Also, depending on where at the top of the mountain you initial start from (ie. initial values of the function), you might end up following a different path. Similarly, depending on the speed at the river climbs down (ie. the learning rate or step size for the gradient descent algorithm), you might arrive at the final destination in a different manner. Both of these criteria can affect whether you fall into a pit (local minima) or are able to avoid it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize a simple gradient descent algorithm and how it works. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "for i in range(nb_epochs):\n",
    "  params_grad = evaluate_gradient(loss_function, data, params)\n",
    "  params = params - learning_rate * params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    # x = x - learning_rate * gradient_of_x\n",
    "    x = x - (gradx * learning_rate)\n",
    "    \n",
    "    # Return the new value for x\n",
    "    return x\n",
    "\n",
    "# Random number between 0 and 10,000. Feel free to set x whatever you like.\n",
    "random.seed(123)\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.1\n",
    "epochs = 100\n",
    "cost_list = []\n",
    "gradx_list = []\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    cost_list.append(cost)\n",
    "    gradx_list.append(gradx)\n",
    "    #print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'gradient')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Lets visualize it\n",
    "# Create two subplots and unpack the output array immediately\n",
    "import matplotlib.pyplot as plt\n",
    "x = range(epochs+1)\n",
    "\n",
    "figure, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))\n",
    "\n",
    "axes[0].plot(x, cost_list)\n",
    "axes[0].set_title(\"cost function\")\n",
    "axes[0].set_xlabel(\"epochs\")\n",
    "axes[0].set_ylabel(\"cost\")\n",
    "#create specific subplots\n",
    "\n",
    "axes[1].plot(x, gradx_list)\n",
    "axes[1].set_title(\"Gradient change\")\n",
    "axes[1].set_xlabel(\"epochs\")\n",
    "axes[1].set_ylabel(\"gradient\")\n",
    "\n",
    "\n",
    "#figure.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below figures illustrates a basic cost function and how it is updated by gradient over the epochs. As once can notice from the figure,  the cost is reduced whilst the gradient approaches to zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Gradient Descent Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Batch Gradient Descent \n",
    "\n",
    "This is a vanilla gradient descent and computes the gradient of the cost function w.r.t. to the parameters \n",
    "$\\theta$ for the entire training dataset:\n",
    "$$\\color{orange}{\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta)\\label{eq:batch_sgd}\\tag{3}}$$\n",
    "\n",
    "where $\\eta $ is ***learning rate***.\n",
    "\n",
    "In code level, we represent the equation  $\\eqref{eq:batch_sgd}$ as follows:\n",
    "\n",
    "```\n",
    "for i in range(nb_epochs):\n",
    "  params_grad = evaluate_gradient(loss_function, data, params)\n",
    "  params = params - learning_rate * params_grad\n",
    "```\n",
    "\n",
    "As we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don't fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Stochastic gradient descent\n",
    "\n",
    "In contrast to batch gradient descent, stochastic gradient descent (SGD) updates  for each training example \n",
    "$x^{(i)}$ and label $y^{(i)}$:\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i)}; y^{(i)})$$\n",
    "\n",
    "SGD performs one update at a time. It is therefore usually much faster and can also be used to learn online.\n",
    "SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "  # shuffle data for better convergence\n",
    "  np.random.shuffle(data)\n",
    "  for example in data:\n",
    "    params_grad = evaluate_gradient(loss_function, example, params)\n",
    "   params = params - learning_rate * params_grad\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Minibatch gradient descent\n",
    "\n",
    "As it mentioned above, minibatch gradient descent performs an update for every mini-batch of \n",
    "$n$ training examples.\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\cdot \\nabla_\\theta J( \\theta; x^{(i:i+n)}; y^{(i:i+n)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "for i in range(nb_epochs):\n",
    "  np.random.shuffle(data)\n",
    "  for batch in get_batches(data, batch_size=32):\n",
    "    params_grad = evaluate_gradient(loss_function, batch, params)\n",
    "    params = params - learning_rate * params_grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Gradient Descent Optimization Methods\n",
    "In this section, we will cover the optimization methods used for gradient descent algorithms to solve following issues:\n",
    "* Choosing a proper learning rate \n",
    "* Learning rate schedules for adjusting the learning rate during training\n",
    "* using same learning rate for all parameters update regardless of having sparse features\n",
    "\n",
    "[TODO] add more from the papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Momentum\n",
    "SGD has trouble navigating ravines, i.e. areas where the surface curves much more steeply in one dimension than in another, which are common around local optima. This problem is solved by introducing momentum inspired from physics. \n",
    "Momentum helps accelerate SGD in the relevant direction and dampens oscillations.\n",
    "It does this by adding a fraction  $\\gamma$  of the update vector of the past time step to the current update vector:\n",
    "\n",
    "$$ \\begin{align} \n",
    "\\begin{split} \n",
    "v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta) \\\\ \n",
    "\\theta &= \\theta - v_t \n",
    "\\end{split} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The momentum term \n",
    "$\\gamma$\n",
    " is usually set to 0.9 or a similar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively speaking, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way (until it reaches its terminal velocity if there is air resistance). Therefore during the parameter update,he momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Nesterov accelerated gradient\n",
    "\n",
    "There is a problem when a ball that rolls down a hill, blindly following the slope. It would be wise to use a ball that has a notion of where it is going so that it knows to slow down before the hill slopes up again.\n",
    "\n",
    "Nesterov accelerated gradient (NAG)is a way to give our momentum term this kind of prescience. We know that we will use our momentum term $\\gamma v_{t-1}$ to move the parameters $\\theta$. Computing $\\theta - \\gamma v_{t-1}$\n",
    " thus gives us an approximation of the next position of the parameters (the gradient is missing for the full update), a rough idea where our parameters are going to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align} \n",
    "\\begin{split} \n",
    "v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\gamma v_{t-1} ) \\\\ \n",
    "\\theta &= \\theta - v_t \n",
    "\\end{split} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aforementioned techniques are to adapt our updates to the slope of our error function and speed up SGD in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Algorithms with Adaptive Learning Rate\n",
    "\n",
    "In this section we will cover the algorithms used for adaptive learning rate.\n",
    "\n",
    "\n",
    "#### 4.4.1 AdaGrad\n",
    "AdaGrad adapts the learning rate in such a way that it performs smaller updates (low learning rates) for parameters associated with frequently occurring features, and larger updates (high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data. The algorithm to perform learning rate update is shown below.\n",
    "\n",
    "<img src=\"fig/adagrad.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2 RMSProp\n",
    "\n",
    "RMSprop is an another wildely used adaptive learning rate method.It tries to resolve Adagrad's radically diminishing learning rates.\n",
    "\n",
    "<img src=\"fig/rmsprop.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3 Adam\n",
    "Adaptive Moment Estimation (Adam) is another method for adaptive learning rates for each parameter. Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface.  Adam works well in practice and compares favorably to other adaptive learning-method algorithms.\n",
    "\n",
    "<img src=\"fig/adam.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to aforementioned optimization algorithms for gradient update, there is also few other algorithms used in a same manner. Which are: \n",
    "* Adadelta\n",
    "* AdaMax\n",
    "* Nadam\n",
    "* AMSGrad\n",
    "* AdamW\n",
    "* QHAdam  \n",
    "* AggMo "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison of each optimization methods mentioned above, please check [this blog](https://johnchenresearch.github.io/demon/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Right Optimization\n",
    "\n",
    "Unfortunately there is no rule of thumb here about how to choose the best optimization methods.  Currently, the most popular optimization algorithms actively in use include SGD, SGD with momentum, RMSProp, RMSProp with momentum, AdaDelta and Adam.While training neural nets, it is a trial and error process to find out right optimization methods for a particular task. \n",
    "\n",
    "Refer to [this notebook](#https://nbviewer.jupyter.org/github/ilguyi/optimizers.numpy/blob/master/optimizer.tf.all.opt.plot.ipynb) for a behavior of each optimization methods. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Initialization Strategies\n",
    "The parameter initialization step can be critical to the model’s ultimate performance, and it requires the right method. There are two way that we can initialize parameters. However, each of them brings issues together. \n",
    "\n",
    "1. Zero Initialization: All weights are initialized with value of zero. That one causes a serious issue which is that If all the weights are initialized with 0, the derivative with respect to loss function is the same for every weight, thus all weights have the same value in subsequent iterations.\n",
    "\n",
    "\n",
    "2. Random Initialization: The parameters (weights) are initialized with random numbers in this approach. However, it has following issues: \n",
    "\n",
    "    * If weights are initialized with very high values the term **np.dot(W,X)+b** becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time. Exploding gradients — This is the exact opposite of vanishing gradients. \n",
    "    * If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as vanishing gradient.\n",
    "    \n",
    "To solve the problems associated with above mentioned approaches, there exists new methods to initialize parameters. The mostly used methods are  He initialization, Xavier initialization. Refer to [deeplearning.ai](https://www.deeplearning.ai/ai-notes/initialization/) blog for comprehensive comparison."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit",
   "language": "python",
   "name": "python361064bit44046df9ee9f4b6794be314555d2632c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
